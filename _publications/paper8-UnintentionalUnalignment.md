---
title: "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
authors: "Noam Razin, Sadhika Malladi, **Adithya Bhaskar**, Danqi Chen, Sanjeev Arora, and Boris Hanin"
collection: publications
permalink: https://arxiv.org/abs/2410.08847
info: "[[paper]](https://arxiv.org/abs/2410.08847) [[code]](https://github.com/princeton-nlp/unintentional-unalignment)"
excerpt: "Sometimes, preference optimzation leads to the a *reduction* in the likelihood of the preferred responses. We shed light on this curious phenomenon."
date: 2024-12-01
venue: 'ICLR 2025'
---